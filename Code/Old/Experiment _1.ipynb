{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first experiment where the model is given all the 98 focal slices to be trained for. The model is also saved for quick use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "autofocus_path='/mnt/Data/Autofocus/'\n",
    "\n",
    "autofocus_train_path=autofocus_path+\"Train/\"\n",
    "autofocus_test_path=autofocus_path+\"Test/\"\n",
    "autofocus_cache_path=autofocus_path+\"Cache/\"\n",
    "autofocus_exp_path=autofocus_path+\"Exp1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset from the Cache folder with only read permision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable loaded successfully from /mnt/Data/Autofocus/Cache/length.dat\n"
     ]
    }
   ],
   "source": [
    "def load_variable_from_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            variable = file.read()\n",
    "        print(f\"Variable loaded successfully from {filename}\")\n",
    "        return variable\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading variable from {filename}: {e}\")\n",
    "        return None\n",
    "    \n",
    "_dataset_len=int(load_variable_from_file(autofocus_cache_path+\"length.dat\"))\n",
    "\n",
    "shutil.copy(autofocus_cache_path+\"dataset.dat\",autofocus_exp_path+\"dataset.dat\")\n",
    "shutil.copy(autofocus_cache_path+\"labels.dat\",autofocus_exp_path+\"labels.dat\")\n",
    "shutil.copy(autofocus_cache_path+\"patch.dat\",autofocus_exp_path+\"patch.dat\")\n",
    "\n",
    "dataset=np.memmap(autofocus_exp_path+\"dataset.dat\", dtype='int8', mode='r', shape=(_dataset_len,128,128,98))\n",
    "labels=np.memmap(autofocus_exp_path+\"labels.dat\", dtype='int8', mode='r', shape=(_dataset_len,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incase the dataset is too large, this function effectively uses every kind of image scene with a reduced patch size making the dataset smaller. The highest threshold value is 15, anything below that will decrease the dataset by that percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10772/10772 [25:52<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable saved successfully to /mnt/Data/Autofocus/Exp1/length.dat\n"
     ]
    }
   ],
   "source": [
    "def patch_threshold (patch_count,threshold):\n",
    "    patch_index=[]\n",
    "    index=0\n",
    "\n",
    "    for count in patch_count:\n",
    "        indices=[index+i for i in range(min(threshold,count))]\n",
    "        index+=count\n",
    "        patch_index=patch_index+indices\n",
    "    \n",
    "    return patch_index\n",
    "\n",
    "def finalize_files(file_path):\n",
    "    try:\n",
    "        os.remove(file_path+\"dataset.dat\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error deleting file: {e}\")\n",
    "\n",
    "    try:\n",
    "        os.rename(file_path+\"dataset_temp.dat\", file_path+\"dataset.dat\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error renaming file: {e}\")\n",
    "\n",
    "    try:\n",
    "        os.remove(file_path+\"labels.dat\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error deleting file: {e}\")\n",
    "\n",
    "    try:\n",
    "        os.rename(file_path+\"labels_temp.dat\", file_path+\"labels.dat\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error renaming file: {e}\")\n",
    "\n",
    "def dataset_threshold(file_path,length,threshold):\n",
    "    dataset=np.memmap(file_path+\"dataset.dat\", dtype='int8', mode='r', shape=(length,128,128,98))\n",
    "    labels=np.memmap(file_path+\"labels.dat\", dtype='int8', mode='r', shape=(length,))\n",
    "    patch_count=np.memmap(file_path+\"patch.dat\", dtype='int8', mode='r', shape=(1775,))\n",
    "\n",
    "    patch_index=patch_threshold(patch_count,threshold)\n",
    "    new_len=len(patch_index)\n",
    "\n",
    "    updated_dataset=np.memmap(file_path+\"dataset_temp.dat\", dtype='int8', mode='w+', shape=(new_len,128,128,98))\n",
    "    updated_labels=np.memmap(file_path+\"labels_temp.dat\", dtype='int8', mode='w+', shape=(new_len,))\n",
    "\n",
    "    for pos,index in tqdm(enumerate(patch_index),total=len(patch_index)):\n",
    "        updated_dataset[pos]=dataset[index]\n",
    "        updated_labels[pos]=labels[index]\n",
    "        updated_dataset.flush()\n",
    "        updated_labels.flush()\n",
    "\n",
    "    dataset.flush()\n",
    "    labels.flush()\n",
    "    patch_count.flush()\n",
    "    updated_dataset.flush()\n",
    "    updated_labels.flush()\n",
    "\n",
    "    finalize_files(file_path)\n",
    "\n",
    "    return new_len\n",
    "\n",
    "\n",
    "dataset.flush()\n",
    "labels.flush()\n",
    "\n",
    "_dataset_len=dataset_threshold(autofocus_exp_path,_dataset_len,7)\n",
    "\n",
    "def save_variable_to_file(variable, filename):\n",
    "    try:\n",
    "        with open(filename, 'w+') as file:\n",
    "            file.write(str(variable))\n",
    "        print(f\"Variable saved successfully to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving variable to {filename}: {e}\")\n",
    "\n",
    "save_variable_to_file(_dataset_len,autofocus_exp_path+\"length.dat\")\n",
    "\n",
    "\n",
    "dataset=np.memmap(autofocus_exp_path+\"dataset.dat\", dtype='int8', mode='r', shape=(_dataset_len,128,128,98))\n",
    "labels=np.memmap(autofocus_exp_path+\"labels.dat\", dtype='int8', mode='r', shape=(_dataset_len,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the Test and Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable loaded successfully from /mnt/Data/Autofocus/Exp1/length.dat\n"
     ]
    }
   ],
   "source": [
    "def load_variable_from_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            variable = file.read()\n",
    "        print(f\"Variable loaded successfully from {filename}\")\n",
    "        return variable\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading variable from {filename}: {e}\")\n",
    "        return None\n",
    "    \n",
    "_dataset_len=int(load_variable_from_file(autofocus_exp_path+\"length.dat\"))\n",
    "\n",
    "shutil.copy(autofocus_cache_path+\"dataset.dat\",autofocus_exp_path+\"dataset.dat\")\n",
    "shutil.copy(autofocus_cache_path+\"labels.dat\",autofocus_exp_path+\"labels.dat\")\n",
    "shutil.copy(autofocus_cache_path+\"patch.dat\",autofocus_exp_path+\"patch.dat\")\n",
    "\n",
    "dataset=np.memmap(autofocus_exp_path+\"dataset.dat\", dtype='int8', mode='r', shape=(_dataset_len,128,128,98))\n",
    "labels=np.memmap(autofocus_exp_path+\"labels.dat\", dtype='int8', mode='r', shape=(_dataset_len,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable loaded successfully from /mnt/Data/Autofocus/Test/Cache/length.dat\n"
     ]
    }
   ],
   "source": [
    "autofocus_test_path_cache=autofocus_test_path+\"Cache/\"\n",
    "\n",
    "shutil.copy(autofocus_test_path_cache+\"dataset.dat\",autofocus_exp_path+\"Test/dataset.dat\")\n",
    "shutil.copy(autofocus_test_path_cache+\"labels.dat\",autofocus_exp_path+\"Test/labels.dat\")\n",
    "\n",
    "_test_len=int(load_variable_from_file(autofocus_test_path_cache+\"length.dat\"))\n",
    "\n",
    "test_data=np.memmap(autofocus_exp_path+\"Test/dataset.dat\", dtype='int8', mode='r', shape=(_test_len,128,128,98))\n",
    "test_labels=np.memmap(autofocus_exp_path+\"Test/labels.dat\", dtype='int8', mode='r', shape=(_test_len,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MobileNetV2 model is used. The input shape is converted to (128,128,98) to be able to take the dual pixel focal stack slices as individual channels of an image. Then we use ordinal regression loss (L2). The Adam optimizer is used to build the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Modify MobileNetV2 to accept 128x128x98 input\n",
    "def create_modified_mobilenetv2(input_shape=(128, 128, 98)):\n",
    "    base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights=None)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # Assuming the output for the ordinal regression problem is a single value\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Step 2: Implement the ordinal regression loss\n",
    "def ordinal_regression_loss(y_true, y_pred):\n",
    "    # L2 loss (mean squared error)\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n",
    "\n",
    "# Step 3: Set up the training loop\n",
    "def train_model(model, train_dataset, steps_per_epoch=20000, epochs=128, initial_lr=1e-5, beta1=0.5, beta2=0.999):\n",
    "    optimizer = Adam(learning_rate=initial_lr, beta_1=beta1, beta_2=beta2)\n",
    "    model.compile(optimizer=optimizer, loss=ordinal_regression_loss,metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# Create the model\n",
    "input_shape = (128, 128, 98)\n",
    "model = create_modified_mobilenetv2(input_shape=input_shape)\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps_per_epoch is the only thing the user running the model needs to think about. The formula is len(dataset)=batch_size * steps_per_epoch * epochs. So choose a good steps_per_epoch size. Choosing a higher number means consuming more RAM but less time and choosing a lesser number number means less RAM but more time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Length : 84\n"
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dataset, labels)).batch(batch_size)\n",
    "print(\"Train Dataset Length :\",len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     "Epoch 1/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1001ms/step - accuracy: 0.915 - loss: 2.263\n",
      "Epoch 2/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 960ms/step - accuracy: 0.939 - loss: 1.351\n",
      "Epoch 3/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 956ms/step - accuracy: 0.973 - loss: 2.419\n",
      "Epoch 4/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1034ms/step - accuracy: 0.964 - loss: 2.47\n",
      "Epoch 5/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1024ms/step - accuracy: 0.923 - loss: 2.372\n",
      "Epoch 6/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 911ms/step - accuracy: 0.999 - loss: 1.501\n",
      "Epoch 7/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 911ms/step - accuracy: 0.906 - loss: 2.982\n",
      "Epoch 8/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 917ms/step - accuracy: 0.964 - loss: 1.273\n",
      "Epoch 9/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 905ms/step - accuracy: 0.981 - loss: 1.095\n",
      "Epoch 10/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1021ms/step - accuracy: 0.971 - loss: 2.125\n",
      "Epoch 11/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 959ms/step - accuracy: 0.987 - loss: 1.02\n",
      "Epoch 12/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 910ms/step - accuracy: 0.92 - loss: 1.231\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch=7\n",
    "epochs=len(train_dataset)//steps_per_epoch\n",
    "\n",
    "train_model(model, train_dataset, steps_per_epoch=steps_per_epoch, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(autofocus_exp_path+\"Experiment_1_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Length : 40\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels)).batch(batch_size)\n",
    "print(\"Test Dataset Length :\",len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 0.965 - loss: 0.234\n",
      "Loss: 0.54836130142212, Accuracy: 96.457846284716289%\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_dataset)\n",
    "loss, accuracy = results\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy*100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear the RAM for immediate clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM cleared\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clear_ram():\n",
    "    global_vars = list(globals().keys())  # Get a list of global variable names\n",
    "    vars_to_delete = [var for var in global_vars]\n",
    "    \n",
    "    # Delete selected variables\n",
    "    for var in vars_to_delete:\n",
    "        del globals()[var]\n",
    "    import gc\n",
    "    # Invoke garbage collector\n",
    "    gc.collect()\n",
    "    \n",
    "    # Print confirmation\n",
    "    print('RAM cleared')\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "clear_ram()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
