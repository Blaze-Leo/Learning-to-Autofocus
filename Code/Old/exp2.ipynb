{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test and Train archives webpage - https://learntoautofocus-google.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to your downloaded files. Store the files in a folder say 'Autofocus'. Then create a dirctory 'Train' as Autofocus/Train and store inside them downloaded folders which have name as 'train<number>' as Autofocus/Train/train<number>. Then create a ditrectory called 'Test' as Autofocus/Test and store the downloaded folder 'test' in it as Autofocus/Test/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is made such that the user running the model needs to download the test archive and atleast 1 train dataset. Theres no need to download all train datasets all the increasing the number of train folders will increase accuracy. Copying a train folder and using that as a seperate folder (for example copying train1 and naming it train2 and using it) will not make any difference. Its the same as using only train1 but will of course consume more time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autofocus_path='S:/Personal/Projects/Python/Autofocus/'\n",
    "#autofocus_path='/data/Autofocus/'\n",
    "autofocus_path='/mnt/Velocity Vault/Autofocus/'\n",
    "autofocus_train_path=autofocus_path+\"Train/\"\n",
    "autofocus_test_path=autofocus_path+\"Test/\"\n",
    "autofocus_temp_path=autofocus_path+\"Temp/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import OpenEXR\n",
    "import Imath\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make dictionaries which contain the location of the depth maps and depth confidance maps. Instead of the storing the file itself, we store just the locations so that the the data doesn't use up RAM and instead we can access them whenever needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_depth_map(super_path):\n",
    "    depth_map_path = {}\n",
    "\n",
    "    # Walk through the directory tree\n",
    "    for root, _, files in os.walk(super_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.png'):\n",
    "                full_path = os.path.join(root, filename)\n",
    "                # Remove super_path from the full path to get the relative path\n",
    "                relative_path = os.path.relpath(full_path, super_path)\n",
    "                # Split the relative path into parts (folders)\n",
    "                parts = relative_path.split(os.sep)\n",
    "                parts[-1]=parts[-1][len('result_merged_depth_'):-len('.png')]\n",
    "                \n",
    "                # Initialize nested dictionaries as needed\n",
    "                current_dict = depth_map_path\n",
    "                for part in parts[:-1]:  # Iterate over all parts except the last one (filename)\n",
    "                    current_dict = current_dict.setdefault(part, {})\n",
    "                \n",
    "                # Assign the full path to the deepest nested dictionary\n",
    "                current_dict[parts[-1]] = full_path\n",
    "\n",
    "    return depth_map_path\n",
    "\n",
    "def build_depth_map_confidence(super_path):\n",
    "    depth_map_path = {}\n",
    "\n",
    "    # Walk through the directory tree\n",
    "    for root, _, files in os.walk(super_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.exr'):\n",
    "                full_path = os.path.join(root, filename)\n",
    "                # Remove super_path from the full path to get the relative path\n",
    "                relative_path = os.path.relpath(full_path, super_path)\n",
    "                # Split the relative path into parts (folders)\n",
    "                parts = relative_path.split(os.sep)\n",
    "                parts[-1]=parts[-1][len('result_merged_conf_'):-len('.exr')]\n",
    "                \n",
    "                # Initialize nested dictionaries as needed\n",
    "                current_dict = depth_map_path\n",
    "                for part in parts[:-1]:  # Iterate over all parts except the last one (filename)\n",
    "                    current_dict = current_dict.setdefault(part, {})\n",
    "                \n",
    "                # Assign the full path to the deepest nested dictionary\n",
    "                current_dict[parts[-1]] = full_path\n",
    "\n",
    "    return depth_map_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible that someone hasn't downloaded all the Train forder for some reason, so we cfind how many folders have been downloaded and then load then in the dictionary as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_train_folders(directory):\n",
    "    train_folders = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for dir in dirs:\n",
    "            if dir.startswith('train'):\n",
    "                train_folders.append(os.path.join(root, dir))\n",
    "    return train_folders\n",
    "\n",
    "train_path=find_train_folders(autofocus_train_path)\n",
    "print(len(train_path))\n",
    "depth_map_path={}\n",
    "depth_map_path_temp={}\n",
    "\n",
    "for path in train_path:\n",
    "    depth_map_path_temp=build_depth_map(path+\"/merged_depth\")\n",
    "    depth_map_path.update(depth_map_path_temp)\n",
    "\n",
    "depth_map_confidence_path={}\n",
    "depth_map_confidence_path_temp={}\n",
    "\n",
    "for path in train_path:\n",
    "    depth_map_confidence_path_temp=build_depth_map_confidence(path+\"/merged_conf\")\n",
    "    depth_map_confidence_path.update(depth_map_confidence_path_temp)\n",
    "\n",
    "pprint.pprint(depth_map_path)\n",
    "pprint.pprint(depth_map_confidence_path)\n",
    "\n",
    "print(len(depth_map_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random patches in the image of sizes 32x32 (since we need 128x128 patches from the images but the depth map is 4 times smaller than the actual image) with a distance of 40 around each patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_random_patches(size=(504,378)):\n",
    "    image=np.zeros(size)\n",
    "    patches=[]\n",
    "    for _ in (range(100)):\n",
    "        x=random.randint(16, 487)\n",
    "        y=random.randint(16, 361)\n",
    "\n",
    "        #print(x,y)\n",
    "        \n",
    "        breaker=False\n",
    "\n",
    "        for i in range(x-26,x+26):\n",
    "            for j in range(y-26,y+26):\n",
    "                if i<0 or i>503 or j<0 or j>377 :\n",
    "                    continue\n",
    "                if image[i][j]==1:\n",
    "                    breaker=True\n",
    "                    break\n",
    "            if breaker:\n",
    "                break\n",
    "\n",
    "        if breaker:\n",
    "            continue\n",
    "        \n",
    "        x=x-16\n",
    "        y=y-16\n",
    "\n",
    "        for i in range(x,x+33):\n",
    "            for j in range(y,y+33):\n",
    "                image[i][j]=1\n",
    "\n",
    "        patches.append((x,y))\n",
    "\n",
    "\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the depth maps, we will take every image and generate patches and calculate which of the patches have a higher median confidance and then calculate the median depth for those patches. Then median depth is converted to actual depth which is our predicted focal length. Then we select the slice with the nearest focal length with the predicted focal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_exr(file_path):\n",
    "    exr_file = OpenEXR.InputFile(file_path)\n",
    "    header = exr_file.header()\n",
    "    dw = header['dataWindow']\n",
    "    width = dw.max.x - dw.min.x + 1\n",
    "    height = dw.max.y - dw.min.y + 1\n",
    "    pt = Imath.PixelType(Imath.PixelType.FLOAT)\n",
    "    r = np.frombuffer(exr_file.channel('R', pt), dtype=np.float32)\n",
    "    r.shape = (height, width)\n",
    "    return r\n",
    "\n",
    "def predict_patches(depth_confidence_path):\n",
    "\n",
    "    patch_threshold = 15\n",
    "\n",
    "    depth_map_confidence = load_exr(depth_confidence_path)\n",
    "\n",
    "    all_patches=generate_random_patches()\n",
    "\n",
    "    confidence_patch_blocks=[depth_map_confidence[x:x+32,y:y+32] for (x,y) in all_patches]\n",
    "    median_confidence_patch_block=[np.median(patch.flatten()) for patch in confidence_patch_blocks]\n",
    "\n",
    "    patch_indices=[i for i, value in enumerate(median_confidence_patch_block) if value >= 0.98]\n",
    "\n",
    "    if len(patch_indices)>4:\n",
    "        patch_indices = sorted(range(len(median_confidence_patch_block)), key=lambda i: median_confidence_patch_block[i], reverse=True)[:patch_threshold]\n",
    "\n",
    "    patches=[all_patches[patch_index] for patch_index in patch_indices]\n",
    "\n",
    "    return patches\n",
    "\n",
    "def predict_focal_length(depth_map_path,patch):\n",
    "    depth_map = cv2.imread(depth_map_path, cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "\n",
    "    depth_values=depth_map[patch[0]:patch[0]+32,patch[1]:patch[1]+32]\n",
    "    depth_values=depth_values.flatten()\n",
    "\n",
    "    # Define max and min values\n",
    "    max_depth = 100.0\n",
    "    min_depth = 0.2\n",
    "\n",
    "    depth_map_in_meters = (max_depth * min_depth) / (max_depth - (max_depth - min_depth) * (depth_values / 255.0))\n",
    "\n",
    "    # Compute the median value in the entire depth map\n",
    "    median_depth = np.median(depth_map_in_meters)\n",
    "\n",
    "    final_focus=median_depth*1000\n",
    "\n",
    "    return final_focus\n",
    "\n",
    "slice_focal_length=[3910.92,2289.27,1508.71,1185.83,935.91,801.09,700.37,605.39,546.23,486.87,447.99,407.40,379.91,350.41,329.95,307.54,291.72,274.13,261.53,247.35,237.08,225.41,216.88,207.10,198.18,191.60,183.96,178.29,171.69,165.57,160.99,155.61,150.59,146.81,142.35,138.98,134.99,131.23,127.69,124.99,121.77,118.73,116.40,113.63,110.99,108.47,106.54,104.23,102.01]\n",
    "\n",
    "def find_closest(value, num_list):\n",
    "    closest_value = min(num_list, key=lambda x: abs(x - value))\n",
    "    return closest_value\n",
    "\n",
    "def predict_slice(depth_map_path,depth_confidence_path):\n",
    "    truth=[]\n",
    "\n",
    "    patches=predict_patches(depth_confidence_path)\n",
    "    \n",
    "    for patch in patches:\n",
    "        predicted_focus=predict_focal_length(depth_map_path,patch)\n",
    "        closest_value = find_closest(predicted_focus, slice_focal_length)\n",
    "        true_slice=slice_focal_length.index(closest_value)\n",
    "        # manual annotation\n",
    "        # true_slice=true_slice-1 if true_slice !=0 else true_slice\n",
    "        truth.append((patch[0],patch[1],true_slice))\n",
    "\n",
    "    return truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the Ground Truth dictionary which contains the predicted focal slice and its respective patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ground_truth=copy.deepcopy(depth_map_path)\n",
    "\n",
    "for image_type in tqdm(ground_truth):\n",
    "    for pos in ground_truth[image_type]:\n",
    "        ground_truth[image_type][pos]=predict_slice(depth_map_path[image_type][pos],depth_map_confidence_path[image_type][pos])\n",
    "\n",
    "pprint.pprint(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the average number of patches, increasing the threshold of 4 in the original function will increase the size of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "patch_count=[]\n",
    "\n",
    "for key1 in ground_truth:\n",
    "    for key2 in ground_truth[key1]:\n",
    "        patch_count.append(len(ground_truth[key1][key2]))\n",
    "\n",
    "print(min(patch_count),\" \",np.mean(patch_count),\" \",max(patch_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that takes the image path and returns the image with its own patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_image_paths(super_path):\n",
    "    # Initialize the dictionary\n",
    "    image_path = {}\n",
    "\n",
    "    # Traverse the directory structure\n",
    "    for root, dirs, files in os.walk(super_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.png'):\n",
    "                # Get the relative path from super_path\n",
    "                relative_path = os.path.relpath(root, super_path)\n",
    "                # Split the relative path to get <string> and <integer>\n",
    "                string_part, integer_part = os.path.split(relative_path)\n",
    "                integer_part = int(integer_part)\n",
    "                \n",
    "                # Extract the position from the filename\n",
    "                file_name_parts = file.split('_')\n",
    "                position = file_name_parts[-1].split('.')[0]  # Extract 'bottom', 'top', etc.\n",
    "                \n",
    "                # Construct the full file path\n",
    "                full_file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Populate the dictionary\n",
    "                if string_part not in image_path:\n",
    "                    image_path[string_part] = {}\n",
    "                if integer_part not in image_path[string_part]:\n",
    "                    image_path[string_part][integer_part] = {}\n",
    "                \n",
    "                image_path[string_part][integer_part][position] = full_file_path\n",
    "\n",
    "    return image_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing only the locations of the left and right dual pixel images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "left_image_path = {}\n",
    "left_image_path_temp = {}\n",
    "right_image_path = {}\n",
    "right_image_path_temp = {}\n",
    "\n",
    "train_path = find_train_folders(autofocus_train_path)\n",
    "print(len(train_path))\n",
    "\n",
    "\n",
    "for path in train_path:\n",
    "    left_image_path_temp = generate_image_paths(path+'/raw_up_left_pd')\n",
    "    left_image_path.update(left_image_path_temp)\n",
    "    right_image_path_temp = generate_image_paths(path+'/raw_up_right_pd')\n",
    "    right_image_path.update(right_image_path_temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dictionary contains the path length as [...][<focal_stack_number>][<position>]=path, for ease of creation of the dataset we need to make it into [...][<position>][<focal_stack_number>]=path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def reorder_image_path(image_path):\n",
    "    ip = {}\n",
    "\n",
    "    for key1, level2_dict in image_path.items():\n",
    "        for key2, level3_dict in level2_dict.items():\n",
    "            for key3, value in level3_dict.items():\n",
    "                if key1 not in ip:\n",
    "                    ip[key1] = {}\n",
    "                if key3 not in ip[key1]:\n",
    "                    ip[key1][key3] = []\n",
    "                ip[key1][key3].append(value)\n",
    "\n",
    "    return ip\n",
    "\n",
    "left_image_path=reorder_image_path(left_image_path)\n",
    "right_image_path=reorder_image_path(right_image_path)\n",
    "\n",
    "pprint.pprint(left_image_path)\n",
    "pprint.pprint(right_image_path)\n",
    "print(len(left_image_path))\n",
    "print(len(right_image_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second experiment we use the dual pixel data so the images are loaded sperately creating 49 left dp slices and 49 right dp slices, thus making 98 slices. So we need to make the paths into a single dictionary for ease of creation of the dataset. Then the the paths a stored in a tuple with left dp and right dp images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def experminet_combine(left_path,right_path):\n",
    "    return list(zip(left_path,right_path))\n",
    "\n",
    "def combine_image_paths(left_image_path,right_image_path):\n",
    "    image_paths=copy.deepcopy(left_image_path)\n",
    "    for key1 in image_paths:\n",
    "        for key2 in image_paths[key1]:\n",
    "            image_paths[key1][key2]=experminet_combine(left_image_path[key1][key2],right_image_path[key1][key2])\n",
    "    return image_paths\n",
    "\n",
    "image_paths=combine_image_paths(left_image_path,right_image_path)\n",
    "\n",
    "pprint.pprint(image_paths)\n",
    "print(len(image_paths))\n",
    "print(type(image_paths['apt1_0']['center']))\n",
    "print(len(image_paths['apt1_0']['center']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the images are of format AxBx3 we need to make them into AxB and then to create the dataset we take multiple such images making it sxAxB and converting it into AxBxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_dimension(arr):\n",
    "    # Ensure the array has the correct shape\n",
    "    s=len(arr.shape)\n",
    "    if s!=2:\n",
    "        print(\"Input array must have shape AxB\")\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def combine_last_dimension(arrays):\n",
    "    A = arrays[0].shape[0]  # Assuming all arrays have the same shape AxA\n",
    "    s = len(arrays)\n",
    "    \n",
    "    # Initialize the resulting array with lists\n",
    "    list_array = np.empty((A, A, s), dtype=object)\n",
    "    \n",
    "    # Fill the array with corresponding elements from each input array\n",
    "    for k, array in enumerate(arrays):\n",
    "        list_array[..., k] = array\n",
    "    \n",
    "    return list_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset and the ground truth labels are created correspoing to each patch of each focal stack slice and the dp values are added and the mean value is taken to simulate only the green channel data of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def build_labels(ground_truth):\n",
    "    labels=[]\n",
    "\n",
    "    for image_type in tqdm(ground_truth):\n",
    "        for pos in ground_truth[image_type]:\n",
    "            temp=ground_truth[image_type][pos]\n",
    "            truth=[z for (_,_,z) in temp]\n",
    "\n",
    "            for i in range(len(truth)):\n",
    "                labels.append(truth[i])\n",
    "\n",
    "    return labels\n",
    "\n",
    "labels_RAM=build_labels(ground_truth)\n",
    "\n",
    "# pprint.pprint(labels)\n",
    "print(len(labels_RAM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_patch_count(ground_truth):\n",
    "    patch_count=[]\n",
    "\n",
    "    for image_type in tqdm(ground_truth):\n",
    "        for pos in ground_truth[image_type]:\n",
    "            count=len(ground_truth[image_type][pos])\n",
    "            patch_count.append(count)\n",
    "\n",
    "    return patch_count\n",
    "\n",
    "patch_RAM=build_patch_count(ground_truth)\n",
    "\n",
    "# pprint.pprint(labels)\n",
    "print(len(patch_RAM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "_dataset_len=len(labels_RAM)\n",
    "\n",
    "filename=autofocus_path+\"exp2_labels_cache.dat\"\n",
    "shape=(_dataset_len)\n",
    "labels = np.memmap(filename, dtype='int8', mode='w+', shape=shape)\n",
    "\n",
    "for index,label in enumerate(labels_RAM):\n",
    "    labels[index]=label\n",
    "\n",
    "labels.flush()\n",
    "\n",
    "filename=autofocus_path+\"exp2_patch_cache.dat\"\n",
    "shape=(len(patch_RAM))\n",
    "patch_count = np.memmap(filename, dtype='int8', mode='w+', shape=shape)\n",
    "\n",
    "for index,patch in enumerate(patch_RAM):\n",
    "    patch_count[index]=patch\n",
    "\n",
    "patch_count.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def sleep_with_progress(seconds,index):\n",
    "    print(\"Start Sleeping\",seconds,\"seconds -\",index)\n",
    "    time.sleep(seconds)\n",
    "    print(\"End Sleeping -\",index)\n",
    "\n",
    "def wait_for_file(filepath, check_interval=1):\n",
    "    while not os.path.isfile(filepath):\n",
    "        time.sleep(check_interval)\n",
    "    return 0\n",
    "\n",
    "def build_dataset(filename,shape,image_paths,ground_truth):\n",
    "\n",
    "    wait_for_file(filename)\n",
    "    dataset=np.memmap(filename, dtype='int8', mode='r+', shape=shape)\n",
    "    data_count=0\n",
    "    count=0\n",
    "    sleep_index=60\n",
    "    for image_type in tqdm(ground_truth):\n",
    "        if count%sleep_index==0 and count!=0:\n",
    "            sleep_with_progress(60,count//sleep_index)\n",
    "        count+=1\n",
    "        for pos in ground_truth[image_type]:\n",
    "            temp=ground_truth[image_type][pos]\n",
    "            patches=[(x,y) for (x,y,_) in temp]\n",
    "\n",
    "            try:\n",
    "                all_images=[(check_dimension(cv2.imread(path[0],0)),check_dimension(cv2.imread(path[1],0))) for path in image_paths[image_type][pos]]\n",
    "            except Exception as e:\n",
    "                print(\"Reconnect Drive\")\n",
    "                checker=[(wait_for_file(path[0]),wait_for_file(path[1])) for path in image_paths[image_type][pos]]\n",
    "                all_images=[(check_dimension(cv2.imread(path[0],0)),check_dimension(cv2.imread(path[1],0))) for path in image_paths[image_type][pos]]\n",
    "\n",
    "            all_images=[(image[0]+image[1])//2 for image in all_images]\n",
    "\n",
    "            for i in range(len(patches)):\n",
    "                x=patches[i][0]*4\n",
    "                y=patches[i][1]*4\n",
    "\n",
    "                images=[image[x:x+128,y:y+128]for image in all_images]\n",
    "\n",
    "                image_set=combine_last_dimension(images)\n",
    "                image_array=np.array(image_set,dtype='int8')\n",
    "\n",
    "                wait_for_file(filename)\n",
    "                dataset[data_count]=image_array            \n",
    "                wait_for_file(filename)\n",
    "                dataset.flush()\n",
    "                data_count+=1\n",
    "\n",
    "\n",
    "filename=autofocus_path+\"exp2_dataset_cache.dat\"\n",
    "shape=(_dataset_len,128,128,49)\n",
    "dataset=np.memmap(filename, dtype='int8', mode='w+', shape=shape)\n",
    "\n",
    "build_dataset(filename,shape,image_paths,ground_truth)\n",
    "\n",
    "# pprint.pprint(dataset)\n",
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just checking if the dataset is in the desired shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_shapes(lst):\n",
    "    target_shape = (128, 128, 49)\n",
    "    for element in lst:\n",
    "        if not isinstance(element, np.ndarray) or element.shape != target_shape:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "print(dataset[0].shape)\n",
    "print(check_shapes(dataset))\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dataset_len=22678\n",
    "\n",
    "def save_variable_to_file(variable, filename):\n",
    "    try:\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(str(variable))\n",
    "        print(f\"Variable saved successfully to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving variable to {filename}: {e}\")\n",
    "\n",
    "save_variable_to_file(_dataset_len,autofocus_path+\"exp2_length.dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp1_to_exp2(exp1_filename, exp1_shape, exp2_filename, exp2_shape):\n",
    "    # Open the input memmap file in read-only mode with dtype int8\n",
    "    exp1_dataset = np.memmap(exp1_filename, dtype='int8', mode='r', shape=exp1_shape)\n",
    "    \n",
    "    # Create the output memmap file with the given shape and dtype int8\n",
    "    exp2_dataset = np.memmap(exp2_filename, dtype='int8', mode='w+', shape=exp2_shape)\n",
    "    \n",
    "    # Loop over the array and process in chunks to handle large data efficiently\n",
    "    for i in tqdm(range(exp1_shape[0])):\n",
    "        for j in range(exp1_shape[1]):\n",
    "            for k in range(exp1_shape[2]):\n",
    "                # Reshape and reduce the last dimension by averaging pairs\n",
    "                reshaped = exp1_dataset[i, j, k].reshape(49, 2)\n",
    "                reduced = reshaped.mean(axis=-1).astype('int8')\n",
    "                \n",
    "                # Write the reduced data to the new memmap file\n",
    "                exp2_dataset[i, j, k] = reduced\n",
    "    \n",
    "        # Flush changes to the output file\n",
    "        exp2_dataset.flush()\n",
    "\n",
    "exp1_to_exp2(autofocus_path+\"exp1_dataset_cache.dat\", (_dataset_len,128,128,98), autofocus_path+\"exp2_dataset_cache.dat\", (_dataset_len,128,128,49))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_threshold (patch_count,threshold):\n",
    "    patch_index=[]\n",
    "    index=0\n",
    "\n",
    "    for count in patch_count:\n",
    "        indices=[index+i for i in range(min(threshold,count))]\n",
    "        index+=count\n",
    "        patch_index=patch_index+indices\n",
    "    \n",
    "    return patch_index\n",
    "\n",
    "def dataset_threshold(data_name,shape_data,patch_count_name,shape_patch,threshold):\n",
    "    dataset=np.memmap(data_name, dtype='int8', mode='r', shape=shape_data)\n",
    "    patch_count=np.memmap(patch_count_name, dtype='int8', mode='r', shape=shape_patch)\n",
    "\n",
    "    patch_index=patch_threshold(patch_count,threshold)\n",
    "    new_len=len(patch_index)\n",
    "\n",
    "    updated_dataset=np.memmap(autofocus_path+\"exp2_dataset_cache.dat\", dtype='int8', mode='w+', shape=(new_len,128,128,49))\n",
    "\n",
    "    for pos,index in tqdm(enumerate(patch_index),total=len(patch_index)):\n",
    "        updated_dataset[pos]=dataset[index]\n",
    "\n",
    "#basically 1775 is 355*5 which is number of scenes*5\n",
    "dataset_threshold(autofocus_temp_path+\"exp2_dataset_cache.dat\",(_dataset_len,128,128,49),autofocus_temp_path+\"exp2_patch_cache.dat\",(1775,),7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incase someone is low on RAM (which I was) run this code, this deletes all the global variables thus freeing RAM and loads the needed variables from the previous instance and loads the dataset from the file which takes very little time compared to creating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_variable_from_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            variable = file.read()\n",
    "        print(f\"Variable loaded successfully from {filename}\")\n",
    "        return variable\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading variable from {filename}: {e}\")\n",
    "        return None\n",
    "    \n",
    "_dataset_len=int(load_variable_from_file(autofocus_path+\"exp2_length.dat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['HSA_OVERRIDE_GFX_VERSION'] = '10.3.0'\n",
    "os.environ['TF_GPU_ALLOCATOR']='cuda_malloc_async'\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import OpenEXR\n",
    "import Imath\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(tf.version.VERSION)\n",
    "\n",
    "dataset=np.memmap(autofocus_path+\"exp2_dataset_cache.dat\", dtype='int8', mode='r', shape=(_dataset_len,128,128,49))\n",
    "labels=np.memmap(autofocus_path+\"exp2_labels_cache.dat\", dtype='int8', mode='r', shape=(_dataset_len,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MobileNetV2 model is used. The input shape is converted to (128,128,49) to be able to take the focal stack slices as individual cahnnels of an image. Then we use ordinal regression loss (L2). The Adam optimizer is used to build the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Modify MobileNetV2 to accept 128x128x98 input\n",
    "def create_modified_mobilenetv2(input_shape=(128, 128, 49)):\n",
    "    base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights=None)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # Assuming the output for the ordinal regression problem is a single value\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Step 2: Implement the ordinal regression loss\n",
    "def ordinal_regression_loss(y_true, y_pred):\n",
    "    # L2 loss (mean squared error)\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n",
    "\n",
    "# Step 3: Set up the training loop\n",
    "def train_model(model, train_dataset, steps_per_epoch=20000, epochs=128, initial_lr=1e-3, beta1=0.5, beta2=0.999):\n",
    "    optimizer = Adam(learning_rate=initial_lr, beta_1=beta1, beta_2=beta2)\n",
    "    model.compile(optimizer=optimizer, loss=ordinal_regression_loss)\n",
    "    \n",
    "    model.fit(train_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# Create the model\n",
    "input_shape = (128, 128, 49)\n",
    "model = create_modified_mobilenetv2(input_shape=input_shape)\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps_per_epoch is the only thing the user running the model needs to think about. The formula is len(dataset)=batch_size*steps_per_epoch*epochs. So choose a good steps_per_epoch size. Choosing a higher number means consuming more RAM but less time and choosing a lesser number number means less RAM but more time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dataset, labels)).batch(batch_size)\n",
    "print(\"Train Dataset Length :\",len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "steps_per_epoch=3\n",
    "epochs=len(train_dataset)//steps_per_epoch\n",
    "\n",
    "train_model(model, train_dataset, steps_per_epoch=steps_per_epoch, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model.save(autofocus_path+\"Experiment_2_model.keras\")\n",
    "\n",
    "#model=tf.keras.models.load_model(autofocus_path+\"autofocus.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def clear_ram():\n",
    "    global_vars = list(globals().keys())  # Get a list of global variable names\n",
    "    vars_to_delete = [var for var in global_vars]\n",
    "    \n",
    "    # Delete selected variables\n",
    "    for var in vars_to_delete:\n",
    "        del globals()[var]\n",
    "    import gc\n",
    "    # Invoke garbage collector\n",
    "    gc.collect()\n",
    "    \n",
    "    # Print confirmation\n",
    "    print('RAM cleared')\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "clear_ram()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
