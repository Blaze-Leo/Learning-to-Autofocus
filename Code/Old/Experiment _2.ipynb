{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the second experiment where the model is given 49 focal slices to be trained for. The model is also saved for quick use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "autofocus_path='/mnt/Data/Autofocus/'\n",
    "\n",
    "autofocus_train_path=autofocus_path+\"Train/\"\n",
    "autofocus_test_path=autofocus_path+\"Test/\"\n",
    "autofocus_cache_path=autofocus_path+\"Cache/\"\n",
    "autofocus_exp_path=autofocus_path+\"Exp2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset from the Cache folder with only read permision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_variable_from_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            variable = file.read()\n",
    "        print(f\"Variable loaded successfully from {filename}\")\n",
    "        return variable\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading variable from {filename}: {e}\")\n",
    "        return None\n",
    "    \n",
    "_dataset_len=int(load_variable_from_file(autofocus_cache_path+\"length.dat\"))\n",
    "\n",
    "\n",
    "shutil.copy(autofocus_cache_path+\"dataset.dat\",autofocus_exp_path+\"dataset.dat\")\n",
    "shutil.copy(autofocus_cache_path+\"labels.dat\",autofocus_exp_path+\"labels.dat\")\n",
    "shutil.copy(autofocus_cache_path+\"patch.dat\",autofocus_exp_path+\"patch.dat\")\n",
    "\n",
    "dataset=np.memmap(autofocus_exp_path+\"dataset.dat\", dtype='int8', mode='r', shape=(_dataset_len,128,128,98))\n",
    "labels=np.memmap(autofocus_exp_path+\"labels.dat\", dtype='int8', mode='r', shape=(_dataset_len,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incase the dataset is too large, this function effectively uses every kind of image scene with a reduced patch size making the dataset smaller. The highest threshold value is 15, anything below that will decrease the dataset by that percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10772/10772 [26:34<00:00,  6.26it/s]\n"
     ]
    }
   ],
   "source": [
    "def patch_threshold (patch_count,threshold):\n",
    "    patch_index=[]\n",
    "    index=0\n",
    "\n",
    "    for count in patch_count:\n",
    "        indices=[index+i for i in range(min(threshold,count))]\n",
    "        index+=count\n",
    "        patch_index=patch_index+indices\n",
    "    \n",
    "    return patch_index\n",
    "\n",
    "def finalize_files(file_path):\n",
    "    try:\n",
    "        os.remove(file_path+\"dataset.dat\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error deleting file: {e}\")\n",
    "\n",
    "    try:\n",
    "        os.rename(file_path+\"dataset_temp.dat\", file_path+\"dataset.dat\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error renaming file: {e}\")\n",
    "\n",
    "    try:\n",
    "        os.remove(file_path+\"labels.dat\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error deleting file: {e}\")\n",
    "\n",
    "    try:\n",
    "        os.rename(file_path+\"labels_temp.dat\", file_path+\"labels.dat\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error renaming file: {e}\")\n",
    "\n",
    "def dataset_threshold(file_path,length,threshold):\n",
    "    dataset=np.memmap(file_path+\"dataset.dat\", dtype='int8', mode='r', shape=(length,128,128,98))\n",
    "    labels=np.memmap(file_path+\"labels.dat\", dtype='int8', mode='r', shape=(length,))\n",
    "    patch_count=np.memmap(file_path+\"patch.dat\", dtype='int8', mode='r', shape=(1775,))\n",
    "\n",
    "    patch_index=patch_threshold(patch_count,threshold)\n",
    "    new_len=len(patch_index)\n",
    "\n",
    "    updated_dataset=np.memmap(file_path+\"dataset_temp.dat\", dtype='int8', mode='w+', shape=(new_len,128,128,98))\n",
    "    updated_labels=np.memmap(file_path+\"labels_temp.dat\", dtype='int8', mode='w+', shape=(new_len,))\n",
    "\n",
    "    for pos,index in tqdm(enumerate(patch_index),total=len(patch_index)):\n",
    "        updated_dataset[pos]=dataset[index]\n",
    "        updated_labels[pos]=labels[index]\n",
    "        updated_dataset.flush()\n",
    "        updated_labels.flush()\n",
    "\n",
    "\n",
    "    dataset.flush()\n",
    "    labels.flush()\n",
    "    patch_count.flush()\n",
    "    updated_dataset.flush()\n",
    "    updated_labels.flush()\n",
    "\n",
    "    finalize_files(file_path)\n",
    "\n",
    "    return new_len\n",
    "\n",
    "\n",
    "dataset.flush()\n",
    "labels.flush()\n",
    "\n",
    "_dataset_len=dataset_threshold(autofocus_exp_path,_dataset_len,7)\n",
    "\n",
    "def save_variable_to_file(variable, filename):\n",
    "    try:\n",
    "        with open(filename, 'w+') as file:\n",
    "            file.write(str(variable))\n",
    "        print(f\"Variable saved successfully to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving variable to {filename}: {e}\")\n",
    "\n",
    "save_variable_to_file(_dataset_len,autofocus_exp_path+\"length.dat\")\n",
    "\n",
    "\n",
    "dataset=np.memmap(autofocus_exp_path+\"dataset.dat\", dtype='int8', mode='r', shape=(_dataset_len,128,128,98))\n",
    "labels=np.memmap(autofocus_exp_path+\"labels.dat\", dtype='int8', mode='r', shape=(_dataset_len,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable loaded successfully from /mnt/Data/Autofocus/Exp2/length.dat\n"
     ]
    }
   ],
   "source": [
    "def load_variable_from_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            variable = file.read()\n",
    "        print(f\"Variable loaded successfully from {filename}\")\n",
    "        return variable\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading variable from {filename}: {e}\")\n",
    "        return None\n",
    "    \n",
    "_dataset_len=int(load_variable_from_file(autofocus_exp_path+\"length.dat\"))\n",
    "\n",
    "shutil.copy(autofocus_cache_path+\"dataset.dat\",autofocus_exp_path+\"dataset.dat\")\n",
    "shutil.copy(autofocus_cache_path+\"labels.dat\",autofocus_exp_path+\"labels.dat\")\n",
    "shutil.copy(autofocus_cache_path+\"patch.dat\",autofocus_exp_path+\"patch.dat\")\n",
    "\n",
    "dataset=np.memmap(autofocus_exp_path+\"dataset.dat\", dtype='int8', mode='r', shape=(_dataset_len,128,128,98))\n",
    "labels=np.memmap(autofocus_exp_path+\"labels.dat\", dtype='int8', mode='r', shape=(_dataset_len,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is now stored as 98 focal slices, we need to make it 49 focal slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10772/10772 [31:46<00:00,  5.65it/s]\n"
     ]
    }
   ],
   "source": [
    "def exp1_to_exp2(file_path, length):\n",
    "    exp1_dataset = np.memmap(file_path+\"dataset.dat\", dtype='int8', mode='r', shape=(length,128,128,98))\n",
    "    exp2_dataset = np.memmap(file_path+\"dataset_temp.dat\", dtype='int8', mode='w+', shape=(length,128,128,49))\n",
    "\n",
    "    shape=(length,128,128,98)\n",
    "    \n",
    "    for i in tqdm(range(shape[0])):\n",
    "        for j in range(shape[1]):\n",
    "            for k in range(shape[2]):\n",
    "                reshaped = exp1_dataset[i, j, k].reshape(49, 2)\n",
    "                reduced = reshaped.mean(axis=-1).astype('int8')\n",
    "                \n",
    "                exp2_dataset[i, j, k] = reduced\n",
    "    \n",
    "        exp2_dataset.flush()\n",
    "\n",
    "    exp1_dataset.flush()\n",
    "    exp2_dataset.flush()\n",
    "\n",
    "    try:\n",
    "        os.remove(file_path+\"dataset.dat\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error deleting file: {e}\")\n",
    "\n",
    "    try:\n",
    "        os.rename(file_path+\"dataset_temp.dat\", file_path+\"dataset.dat\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error renaming file: {e}\")\n",
    "\n",
    "\n",
    "dataset.flush()\n",
    "\n",
    "exp1_to_exp2(autofocus_exp_path, _dataset_len)\n",
    "\n",
    "dataset=np.memmap(autofocus_exp_path+\"dataset.dat\", dtype='int8', mode='r', shape=(_dataset_len,128,128,49))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable loaded successfully from /mnt/Data/Autofocus/Test/Cache/length.dat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1251/1251 [04:33<00:00,  4.57it/s]\n"
     ]
    }
   ],
   "source": [
    "autofocus_test_path_cache=autofocus_test_path+\"Cache/\"\n",
    "\n",
    "shutil.copy(autofocus_test_path_cache+\"dataset.dat\",autofocus_exp_path+\"Test/dataset.dat\")\n",
    "shutil.copy(autofocus_test_path_cache+\"labels.dat\",autofocus_exp_path+\"Test/labels.dat\")\n",
    "\n",
    "_test_len=int(load_variable_from_file(autofocus_test_path_cache+\"length.dat\"))\n",
    "\n",
    "def exp1_to_exp2(file_path, length):\n",
    "    exp1_dataset = np.memmap(file_path+\"dataset.dat\", dtype='int8', mode='r', shape=(length,128,128,98))\n",
    "    exp2_dataset = np.memmap(file_path+\"dataset_temp.dat\", dtype='int8', mode='w+', shape=(length,128,128,49))\n",
    "\n",
    "    shape=(length,128,128,98)\n",
    "    \n",
    "    for i in tqdm(range(shape[0])):\n",
    "        for j in range(shape[1]):\n",
    "            for k in range(shape[2]):\n",
    "                reshaped = exp1_dataset[i, j, k].reshape(49, 2)\n",
    "                reduced = reshaped.mean(axis=-1).astype('int8')\n",
    "                \n",
    "                exp2_dataset[i, j, k] = reduced\n",
    "    \n",
    "        exp2_dataset.flush()\n",
    "\n",
    "    exp1_dataset.flush()\n",
    "    exp2_dataset.flush()\n",
    "\n",
    "    try:\n",
    "        os.remove(file_path+\"dataset.dat\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error deleting file: {e}\")\n",
    "\n",
    "    try:\n",
    "        os.rename(file_path+\"dataset_temp.dat\", file_path+\"dataset.dat\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error renaming file: {e}\")\n",
    "\n",
    "exp1_to_exp2(autofocus_exp_path+\"Test/\", _test_len)\n",
    "\n",
    "\n",
    "test_data=np.memmap(autofocus_exp_path+\"Test/dataset.dat\", dtype='int8', mode='r', shape=(_test_len,128,128,49))\n",
    "test_labels=np.memmap(autofocus_exp_path+\"Test/labels.dat\", dtype='int8', mode='r', shape=(_test_len,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the Test and Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable loaded successfully from /mnt/Data/Autofocus/Exp2/length.dat\n"
     ]
    }
   ],
   "source": [
    "def load_variable_from_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            variable = file.read()\n",
    "        print(f\"Variable loaded successfully from {filename}\")\n",
    "        return variable\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading variable from {filename}: {e}\")\n",
    "        return None\n",
    "    \n",
    "_dataset_len=int(load_variable_from_file(autofocus_exp_path+\"length.dat\"))\n",
    "\n",
    "dataset=np.memmap(autofocus_exp_path+\"dataset.dat\", dtype='int8', mode='r', shape=(_dataset_len,128,128,49))\n",
    "labels=np.memmap(autofocus_exp_path+\"labels.dat\", dtype='int8', mode='r', shape=(_dataset_len,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable loaded successfully from /mnt/Data/Autofocus/Test/Cache/length.dat\n"
     ]
    }
   ],
   "source": [
    "autofocus_test_path_cache=autofocus_test_path+\"Cache/\"\n",
    "\n",
    "shutil.copy(autofocus_test_path_cache+\"dataset.dat\",autofocus_exp_path+\"Test/dataset.dat\")\n",
    "shutil.copy(autofocus_test_path_cache+\"labels.dat\",autofocus_exp_path+\"Test/labels.dat\")\n",
    "\n",
    "_test_len=int(load_variable_from_file(autofocus_test_path_cache+\"length.dat\"))\n",
    "\n",
    "test_data=np.memmap(autofocus_exp_path+\"Test/dataset.dat\", dtype='int8', mode='r', shape=(_test_len,128,128,49))\n",
    "test_labels=np.memmap(autofocus_exp_path+\"Test/labels.dat\", dtype='int8', mode='r', shape=(_test_len,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MobileNetV2 model is used. The input shape is converted to (128,128,49) to be able to take the the green channel of an image which is the integer mean of respective dual pixel focal stack slices of an image. Then we use ordinal regression loss (L2). The Adam optimizer is used to build the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Modify MobileNetV2 to accept 128x128x49 input\n",
    "def create_modified_mobilenetv2(input_shape=(128, 128, 49)):\n",
    "    base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights=None)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # Assuming the output for the ordinal regression problem is a single value\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Step 2: Implement the ordinal regression loss\n",
    "def ordinal_regression_loss(y_true, y_pred):\n",
    "    # L2 loss (mean squared error)\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n",
    "\n",
    "# Step 3: Set up the training loop\n",
    "def train_model(model, train_dataset, steps_per_epoch=20000, epochs=128, initial_lr=1e-7, beta1=0.5, beta2=0.999):\n",
    "    optimizer = Adam(learning_rate=initial_lr, beta_1=beta1, beta_2=beta2)\n",
    "    model.compile(optimizer=optimizer, loss=ordinal_regression_loss,metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# Create the model\n",
    "input_shape = (128, 128, 49)\n",
    "model = create_modified_mobilenetv2(input_shape=input_shape)\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps_per_epoch is the only thing the user running the model needs to think about. The formula is len(dataset)=batch_size * steps_per_epoch * epochs. So choose a good steps_per_epoch size. Choosing a higher number means consuming more RAM but less time and choosing a lesser number number means less RAM but more time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Length : 84\n"
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dataset, labels)).batch(batch_size)\n",
    "print(\"Train Dataset Length :\",len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 914ms/step - accuracy: 0.908 - loss: 1.169\n",
      "Epoch 2/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 997ms/step - accuracy: 0.967 - loss: 1.367\n",
      "Epoch 3/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1007ms/step - accuracy: 0.997 - loss: 1.572\n",
      "Epoch 4/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1013ms/step - accuracy: 0.933 - loss: 1.524\n",
      "Epoch 5/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1000ms/step - accuracy: 0.957 - loss: 2.821\n",
      "Epoch 6/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 983ms/step - accuracy: 0.983 - loss: 2.888\n",
      "Epoch 7/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 907ms/step - accuracy: 0.918 - loss: 2.812\n",
      "Epoch 8/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 921ms/step - accuracy: 0.912 - loss: 1.562\n",
      "Epoch 9/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 981ms/step - accuracy: 0.955 - loss: 1.36\n",
      "Epoch 10/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1032ms/step - accuracy: 0.959 - loss: 2.539\n",
      "Epoch 11/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 976ms/step - accuracy: 0.911 - loss: 1.695\n",
      "Epoch 12/12\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 945ms/step - accuracy: 0.961 - loss: 2.758\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch=7\n",
    "epochs=len(train_dataset)//steps_per_epoch\n",
    "\n",
    "train_model(model, train_dataset, steps_per_epoch=steps_per_epoch, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Length : 40\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels)).batch(batch_size)\n",
    "print(\"Test Dataset Length :\",len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(autofocus_exp_path+\"Experiment_2_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 0.952 - loss: 0.461\n",
      "Loss: 0.461847284982022, Accuracy: 95.273894927556324%\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_dataset)\n",
    "loss, accuracy = results\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy*100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear the RAM for immediate clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM cleared\n"
     ]
    }
   ],
   "source": [
    "def clear_ram():\n",
    "    global_vars = list(globals().keys())  # Get a list of global variable names\n",
    "    vars_to_delete = [var for var in global_vars]\n",
    "    \n",
    "    # Delete selected variables\n",
    "    for var in vars_to_delete:\n",
    "        del globals()[var]\n",
    "    import gc\n",
    "    # Invoke garbage collector\n",
    "    gc.collect()\n",
    "    \n",
    "    # Print confirmation\n",
    "    print('RAM cleared')\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "clear_ram()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
