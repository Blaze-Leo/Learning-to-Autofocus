{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import OpenEXR\n",
    "import Imath\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_org='/mnt/Velocity_Vault/Autofocus/Test'\n",
    "train_org='/mnt/Velocity_Vault/Autofocus/Train/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Train subfolders ( if needed reduce dataset size here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/Velocity_Vault/Autofocus/Train/train1',\n",
      " '/mnt/Velocity_Vault/Autofocus/Train/train2',\n",
      " '/mnt/Velocity_Vault/Autofocus/Train/train3',\n",
      " '/mnt/Velocity_Vault/Autofocus/Train/train4',\n",
      " '/mnt/Velocity_Vault/Autofocus/Train/train5',\n",
      " '/mnt/Velocity_Vault/Autofocus/Train/train6',\n",
      " '/mnt/Velocity_Vault/Autofocus/Train/train7']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_subfolder_paths(path):\n",
    "    if not os.path.isdir(path):\n",
    "        raise ValueError(f\"The provided path '{path}' is not a valid directory.\")\n",
    "    \n",
    "    subfolders = [\n",
    "        os.path.join(path, name) \n",
    "        for name in os.listdir(path) \n",
    "        if os.path.isdir(os.path.join(path, name))\n",
    "    ]\n",
    "    return subfolders\n",
    "\n",
    "train_path=get_subfolder_paths(train_org)\n",
    "test_path=[test_org]\n",
    "\n",
    "\n",
    "# train_path=[train_path[0]]\n",
    "\n",
    "pprint(train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Picture locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 355]\n",
      "[4, 47]\n"
     ]
    }
   ],
   "source": [
    "def get_frame_location(path, upto):\n",
    "    if not os.path.isdir(path):\n",
    "        raise ValueError(f\"The provided path '{path}' is not a valid directory.\")\n",
    "\n",
    "    subfolders = []\n",
    "\n",
    "    def find_subfolders(current_path, level):\n",
    "        if level > upto:  # Stop recursion if the level exceeds 3\n",
    "            return\n",
    "        for item in os.listdir(current_path):\n",
    "            item_path = os.path.join(current_path, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                if level==upto:\n",
    "                    subfolders.append(item_path)\n",
    "                find_subfolders(item_path, level + 1)\n",
    "\n",
    "    find_subfolders(path, 1)\n",
    "    return subfolders\n",
    "\n",
    "# pos -\n",
    "# 0 - raw_left\n",
    "# 1 - raw_right\n",
    "# 2 - depth\n",
    "# 3 - conf\n",
    "\n",
    "def get_image_location(path,pos):\n",
    "    \n",
    "    suffix={\n",
    "        0:\"/raw_up_left_pd\",\n",
    "        1:\"/raw_up_right_pd\",\n",
    "        2:\"/merged_depth\",\n",
    "        3:\"/merged_conf\",\n",
    "    }\n",
    "    \n",
    "    upto={\n",
    "        0:1,\n",
    "        1:1,\n",
    "        2:1,\n",
    "        3:1\n",
    "    }\n",
    "    \n",
    "    raw_left=[]\n",
    "    for p in path:\n",
    "        raw_left+=get_frame_location(p+suffix[pos],upto[pos])\n",
    "        \n",
    "    return raw_left\n",
    "\n",
    "train_image=[[] for _ in range(4)]\n",
    "\n",
    "for i in range(4):\n",
    "    train_image[i]=get_image_location(train_path,i)\n",
    "\n",
    "test_image=[[] for _ in range(4)]\n",
    "\n",
    "for i in range(4):\n",
    "    test_image[i]=get_image_location(test_path,i)\n",
    "\n",
    "def get_list_shape(lst):\n",
    "    if isinstance(lst, list):\n",
    "        return [len(lst)] + get_list_shape(lst[0]) if lst else [0]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "pprint(get_list_shape(train_image))\n",
    "pprint(get_list_shape(test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_with_progress(seconds):\n",
    "    print(\"Start Sleeping\",seconds,\"seconds\")\n",
    "    time.sleep(seconds)\n",
    "    print(\"End Sleeping\")\n",
    "\n",
    "def check_image_connection(image_path):\n",
    "    # Display the \"Reconnect Drive\" message\n",
    "    \n",
    "    print(\"Reconnect Drive\")\n",
    "    checking=True\n",
    "    \n",
    "    while checking:\n",
    "        # Try reading the image\n",
    "        if (not os.path.isfile(image_path)) or (not os.path.isdir('/mnt/Velocity_Vault')):\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            print(image_path)\n",
    "            checking=False\n",
    "    print(\"Drive Connected\")\n",
    "\n",
    "def convert_range(value, old_min=0, old_max=65535, new_min=-1, new_max=1):\n",
    "    # Linear interpolation formula\n",
    "    return ((value - old_min) / (old_max - old_min)) * (new_max - new_min) + new_min\n",
    "\n",
    "\n",
    "def get_image_array(image_path,x,y,change_range=False):\n",
    "    \n",
    "    # Read the image in grayscale\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED) \n",
    "    \n",
    "    if image is None:\n",
    "        check_image_connection(image_path)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "    send=image[x:x+128,y:y+128]\n",
    "    \n",
    "    if change_range:\n",
    "        send=convert_range(send)\n",
    "        \n",
    "    #print(image.shape)\n",
    "    \n",
    "    return send\n",
    "\n",
    "\n",
    "def load_depth_image(image_path,x,y):\n",
    "    # Load standard image format (e.g., PNG, JPG)\n",
    "    depth_array = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "    if depth_array is None:\n",
    "        check_image_connection(image_path)\n",
    "        depth_array = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    # Convert to float32 for consistency\n",
    "    depth_array = np.float32(depth_array)\n",
    "\n",
    "    return depth_array[x:x+32, y:y+32]\n",
    "\n",
    "def load_conf_image(exr_path, x,y,channel=\"R\"):\n",
    "    \n",
    "    try:\n",
    "        exr_file = OpenEXR.InputFile(exr_path)\n",
    "    except Exception as e:\n",
    "        check_image_connection(exr_path)\n",
    "        exr_file = OpenEXR.InputFile(exr_path)\n",
    "        \n",
    "    header = exr_file.header()\n",
    "    dw = header['dataWindow']\n",
    "    width = dw.max.x - dw.min.x + 1\n",
    "    height = dw.max.y - dw.min.y + 1\n",
    "    pixel_type = Imath.PixelType(Imath.PixelType.FLOAT)  \n",
    "    channel_data = exr_file.channel(channel, pixel_type)\n",
    "    np_array = np.frombuffer(channel_data, dtype=np.float32).reshape((height, width))\n",
    "    \n",
    "    return np_array[x:x+32, y:y+32]\n",
    "\n",
    "\n",
    "def approx_depth(depth, conf):\n",
    "    \n",
    "    depth_values = depth.flatten()\n",
    "    confidence_values = conf.flatten()\n",
    "\n",
    "    # Compute the weighted average depth\n",
    "    weighted_sum = np.sum(depth_values * confidence_values)\n",
    "    total_confidence = np.sum(confidence_values)\n",
    "    \n",
    "    approximate_depth = weighted_sum / total_confidence\n",
    "\n",
    "    return approximate_depth\n",
    "\n",
    "\n",
    "def find_slice(desc_list, number):\n",
    "    if not desc_list:\n",
    "        raise ValueError(\"The list cannot be empty.\")\n",
    "    if not all(desc_list[i] >= desc_list[i + 1] for i in range(len(desc_list) - 1)):\n",
    "        raise ValueError(\"The list must be in descending order.\")\n",
    "    \n",
    "    closest_index = min(range(len(desc_list)), key=lambda i: abs(desc_list[i] - number))\n",
    "    return closest_index\n",
    "\n",
    "\n",
    "def predict_slice(depth):\n",
    "    \n",
    "    approx=depth/255.0\n",
    "\n",
    "    max_val=3.9\n",
    "    min_val=0.1\n",
    "\n",
    "    metre=(max_val * min_val) / (max_val - (max_val - min_val) * approx)\n",
    "    metre*=1000\n",
    "\n",
    "    slice_focal_length=[3910.92,2289.27,1508.71,1185.83,935.91,801.09,700.37,605.39,546.23,486.87,447.99,407.40,379.91,350.41,329.95,307.54,291.72,274.13,261.53,247.35,237.08,225.41,216.88,207.10,198.18,191.60,183.96,178.29,171.69,165.57,160.99,155.61,150.59,146.81,142.35,138.98,134.99,131.23,127.69,124.99,121.77,118.73,116.40,113.63,110.99,108.47,106.54,104.23,102.01]\n",
    "\n",
    "    slice_focus=find_slice(slice_focal_length,metre)\n",
    "    \n",
    "    return slice_focus\n",
    "    \n",
    "def create_patches(n):\n",
    "    \n",
    "    def generate_positions(size, clear, countx,county):\n",
    "        positions=[]\n",
    "        pad=clear//2\n",
    "        for i in range(countx):\n",
    "            for j in range(county):\n",
    "                positions.append([i*(size+clear)+pad,j*(size+clear)+pad])\n",
    "        return positions\n",
    "    \n",
    "    positions=generate_positions(128,40,12,9)\n",
    "    \n",
    "    if n > len(positions):\n",
    "        raise ValueError(\"n cannot be greater than the length of the list.\")\n",
    "    \n",
    "    return random.sample(positions, n)\n",
    "\n",
    "def transpose_list(input_list):\n",
    "    np_array = np.array(input_list)\n",
    "    transposed_array = np.transpose(np_array, (1, 2, 0))\n",
    "    return transposed_array\n",
    "\n",
    "def make_stack(left_loc,right_loc,depth_loc,conf_loc):\n",
    "    \n",
    "    slices=np.zeros((12,128, 128,98))\n",
    "    truth=[0 for _ in range(12)]\n",
    "    \n",
    "    patch_pos=create_patches(12)\n",
    "    \n",
    "    for pos,patch in enumerate(patch_pos):\n",
    "        \n",
    "        x=patch[0]\n",
    "        y=patch[1]\n",
    "        \n",
    "        frames=np.zeros((98, 128, 128))\n",
    "        \n",
    "        for i in range(49):\n",
    "            left_frame=left_loc+\"/\"+str(i)+'/result_up_pd_left_center.png'\n",
    "            right_frame=right_loc+\"/\"+str(i)+'/result_up_pd_right_center.png'\n",
    "\n",
    "            frames[i*2]=get_image_array(left_frame,x,y,change_range=True)\n",
    "            frames[i*2+1]=get_image_array(right_frame,x,y,change_range=True)\n",
    "            \n",
    "        # display_map(frames[0])\n",
    "            \n",
    "        slices[pos]=transpose_list(frames)\n",
    "            \n",
    "        depth_frame=depth_loc+'/result_merged_depth_center.png'\n",
    "        conf_frame=conf_loc+'/result_merged_conf_center.exr'\n",
    "        \n",
    "        x//=4\n",
    "        y//=4\n",
    "        \n",
    "        depth=load_depth_image(depth_frame,x,y)\n",
    "        conf=load_conf_image(conf_frame,x,y)\n",
    "        \n",
    "        approx=approx_depth(depth,conf)\n",
    "        focus=predict_slice(approx)\n",
    "        \n",
    "        truth[pos]=focus\n",
    "        \n",
    "        \n",
    "    return slices,truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memmap_path='/mnt/Velocity_Vault/Autofocus/Dataset_Storage/'\n",
    "num_patches=12\n",
    "\n",
    "train_dataset = np.memmap(memmap_path+\"train_dataset\", dtype=np.float64, mode='w+', shape=(num_patches*len(train_image[0]),128, 128,98))\n",
    "train_truth = np.memmap(memmap_path+\"train_truth\", dtype=int, mode='w+', shape=(num_patches*len(train_image[0])))\n",
    "\n",
    "\n",
    "test_dataset = np.memmap(memmap_path+\"test_dataset\", dtype=np.float64, mode='w+', shape=(num_patches*len(test_image[0]),128, 128,98))\n",
    "test_truth = np.memmap(memmap_path+\"test_truth\", dtype=int, mode='w+', shape=(num_patches*len(test_image[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4260, 128, 128, 98)\n",
      "(4260,)\n",
      "(564, 128, 128, 98)\n",
      "(564,)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)\n",
    "print(train_truth.shape)\n",
    "print(test_dataset.shape)\n",
    "print(test_truth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_train_dataset():\n",
    "    for i in tqdm(range(len(train_image[0]))):\n",
    "        \n",
    "        left_loc=train_image[0][i]\n",
    "        right_loc=train_image[1][i]\n",
    "        depth_loc=train_image[2][i]\n",
    "        conf_loc=train_image[3][i]\n",
    "        \n",
    "        slices,truth=make_stack(left_loc,right_loc,depth_loc,conf_loc)\n",
    "        \n",
    "        for j in range(num_patches):\n",
    "            train_dataset[num_patches*i+j]=slices[j]\n",
    "            train_truth[num_patches*i+j]=truth[j]\n",
    "                        \n",
    "def load_test_dataset():\n",
    "    for i in tqdm(range(len(test_image[0]))):\n",
    "        \n",
    "        left_loc=test_image[0][i]\n",
    "        right_loc=test_image[1][i]\n",
    "        depth_loc=test_image[2][i]\n",
    "        conf_loc=test_image[3][i]\n",
    "        \n",
    "        slices,truth=make_stack(left_loc,right_loc,depth_loc,conf_loc)\n",
    "        \n",
    "        for j in range(num_patches):\n",
    "            test_dataset[num_patches*i+j]=slices[j]\n",
    "            test_truth[num_patches*i+j]=truth[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 13/47 [11:16<30:16, 53.44s/it]"
     ]
    }
   ],
   "source": [
    "load_test_dataset()\n",
    "\n",
    "pprint(test_dataset.shape)\n",
    "pprint(test_truth.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_train_dataset()\n",
    "\n",
    "pprint(train_dataset.shape)\n",
    "pprint(train_truth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_truth_one_hot = np.memmap(memmap_path+\"train_truth_one_hot\", dtype=int, mode='w+', shape=(num_patches*len(train_image[0]),49))\n",
    "\n",
    "train_truth_one_hot[np.arange(len(train_truth)),train_truth]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(train_truth.shape)\n",
    "pprint(train_truth_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memmap_path='/mnt/Velocity_Vault/Autofocus/Dataset_Storage/'\n",
    "# num_patches=12\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# train_dataset = np.memmap(memmap_path+\"train_dataset\", dtype=np.float64, mode='r', shape=(num_patches*355,128, 128,98))\n",
    "# train_truth = np.memmap(memmap_path+\"train_truth\", dtype=int, mode='r', shape=(num_patches*355))\n",
    "\n",
    "# ordered_train_dataset = np.memmap(memmap_path+\"ordered_train_dataset\", dtype=np.float64, mode='w+', shape=(num_patches*355,128, 128,98))\n",
    "# ordered_train_truth = np.memmap(memmap_path+\"ordered_train_truth\", dtype=int, mode='w+', shape=(num_patches*355))\n",
    "\n",
    "# sorted_indices = np.argsort(train_truth)\n",
    "# print(sorted_indices.shape)\n",
    "\n",
    "# for i in tqdm(range(len(sorted_indices))):\n",
    "#     ordered_train_dataset[i] = train_dataset[sorted_indices[i]]\n",
    "#     ordered_train_truth[i] = train_truth[sorted_indices[i]]\n",
    "#     ordered_train_dataset.flush()\n",
    "#     ordered_train_truth.flush()\n",
    "    \n",
    "    \n",
    "# from pprint import pprint\n",
    "\n",
    "# pprint(ordered_train_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def plot_occurrences(numbers):\n",
    "    # Count occurrences of each element\n",
    "    occurrences = Counter(numbers)\n",
    "    \n",
    "\n",
    "    # Extract keys (unique numbers) and their corresponding values (counts)\n",
    "    elements = list(occurrences.keys())\n",
    "    counts = list(occurrences.values())\n",
    "    print(np.mean(counts))\n",
    "\n",
    "    # Plot a horizontal bar graph\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.barh(elements, counts, color=\"skyblue\")\n",
    "    plt.xlabel(\"Occurrences\")\n",
    "    plt.ylabel(\"Elements\")\n",
    "    plt.title(\"Occurrences of Elements in the List\")\n",
    "    plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.show()\n",
    "    \n",
    "plot_occurrences(train_truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
